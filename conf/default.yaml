model:

  encoder:
    model_name: "facebook/hubert-large-ls960-ft"
    sampling_rate: 16_000

  decoder:
    model_name: "abacaj/phi-2-super"
    audio_placeholder: null # uses default one
    prompt_template: null # uses default one

    quantization_config:
      load_in_4bit: True
      bnb_4bit_use_double_quant: True
      bnb_4bit_quant_type: nf4
      bnb_4bit_compute_dtype: bfloat16

    peft:
      peft_type: LORA
      r: 8
      lora_alpha: 32
      lora_dropout: 0.05
      bias: none
      target_modules:
        - q_proj
        - k_proj
        - v_proj
        - o_proj
        - gate_proj
        - up_proj
        - down_proj
        - lm_head

datasets:

  librispeech_asr:
    audio_column: audio # TODO: use this value to pre-process it according to encoder SR
    label_column: text # TODO: use this value to pre-process it according to encoder SR
    instruct_column: text # TODO: use this value to pre-process it according to encoder SR
    instruct: "Transcribe speech to text {audio}" # add/overwrite instruct value in dataset
    subset: all

    splits:
      train: train.other.500[:50000]
      test: test.other
      validation: validation.other

    filters:
      is_shorter_than_model_max_length:
        columns: ["labels", "inputs.instruct"]
        safe_padding: 30

  WizardLM/WizardLM_evol_instruct_70k:
    instruct_column: instruction # TODO: use this value to pre-process it according to encoder SR

    label_column: output # TODO: use this value to pre-process it according to encoder SR

    splits:
      train: train[:50000]
      test: train[50000:60000]
      validation: train[60000:]

    filters:
      is_shorter_than_model_max_length:
        columns: ["labels", "inputs.instruct"]
        safe_padding: 30

      is_longer_than:
        columns: ["labels", "inputs.instruct"]
        value: 0

pretraining:
  wandb_project_name: SMIT_ALIGNEMENT
  early_stopping_patience: 5
  resume_from_checkpoint: null
  training_args:
    save_only_model: True
    output_dir: "/scratch/SMIT-alignement-outputs/"
    # group_by_length=True, # Makes the training init super long (~2h)
    bf16: True
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 16
    evaluation_strategy: "steps"
    eval_steps: 1500
    save_steps: 1500
    logging_steps: 100
    learning_rate: 1e-4
    max_steps: 100
    # max_steps: 50_000
    warmup_steps: 500
    save_total_limit: 1
    dataloader_num_workers: 16
    report_to: "wandb"
    weight_decay: 0
    load_best_model_at_end: True

training:
  wandb_project_name: SMIT_TRAINING
  early_stopping_patience: 9
  resume_from_checkpoint: null
  training_args:
    output_dir: "/scratch/SMIT-ASR-outputs/model/"
    # group_by_length=True, # Makes the training init super long (~2h)
    bf16: True
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 2
    per_device_eval_batch_size: 16
    evaluation_strategy: "steps"
    eval_steps: 1000
    save_steps: 1000
    logging_steps: 100
    learning_rate: 1e-4
    max_steps: 100
    # max_steps: 75_000
    warmup_steps: 1_000
    save_total_limit: 10
    dataloader_num_workers: 16
    report_to: "wandb"
    weight_decay: 0
    load_best_model_at_end: True
