defaults:
  - model: example
  - _self_

datasets:

  librispeech_asr:
    audio_column: audio
    label_column: text
    instruct_column: text
    instruct: "Transcribe speech to text {audio}" # add/overwrite instruct value in dataset
    subset: all

    splits:
      train: train.other.500[:130031] # set as 75% of the total data
      test: test.other # set as 75% of the total data
      validation: validation.other # set as 75% of the total data

  WizardLM/WizardLM_evol_instruct_70k:
    instruct_column: instruction # TODO: use this value to pre-process it according to encoder SR

    label_column: output # TODO: use this value to pre-process it according to encoder SR

    splits:
      train: train[:25000] # set as 25% of the total data
      test: train[25000:25979] # set as 25% of the total data
      validation: train[26000:26954] # set as 25% of the total data

    filters:
      is_shorter_than_model_max_length:
        columns: ["labels", "inputs.instruct"]
        safe_padding: 30

      is_longer_than:
        columns: ["labels", "inputs.instruct"]
        value: 0

# Deactivated by default to make the training process faster but you're encouraged to try pretraining
# pretraining:
#   wandb_project_name: SMIT-Pretraining
#   early_stopping_patience: 5
#   resume_from_checkpoint: null
#   training_args:
#     save_only_model: True
#     output_dir: ./outputs/SMIT-Pretraining-outputs/
#     # group_by_length=True # Makes the training init super long (~2h)
#     bf16: True
#     per_device_train_batch_size: 4
#     per_device_eval_batch_size: 16
#     evaluation_strategy: "steps"
#     eval_steps: 1500
#     save_steps: 1500
#     logging_steps: 100
#     learning_rate: 1e-4
#     max_steps: 50_000
#     warmup_steps: 500
#     save_total_limit: 1
#     dataloader_num_workers: 16
#     report_to: "wandb"
#     weight_decay: 0
#     load_best_model_at_end: True

training:
  wandb_project_name: SMIT-Training
  early_stopping_patience: 5
  resume_from_checkpoint: null
  training_args:
    output_dir: "./outputs/SMIT-Training-outputs/"
    # group_by_length=True # Makes the training init super long (~2h)
    bf16: True
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 4
    per_device_eval_batch_size: 1
    evaluation_strategy: "steps"
    eval_steps: 2000
    save_steps: 2000
    logging_steps: 100
    learning_rate: 1e-4
    max_steps: 75_000
    warmup_steps: 1_000
    save_total_limit: 6
    dataloader_num_workers: 16
    report_to: "wandb"
    weight_decay: 0
    load_best_model_at_end: True
