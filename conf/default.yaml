defaults:
  - model: example

datasets:

  librispeech_asr:
    audio_column: audio
    label_column: text
    instruct_column: text
    instruct: "Transcribe speech to text {audio}" # add/overwrite instruct value in dataset
    subset: all

    splits:
      train: train.other.500[:50000]
      test: test.other
      validation: validation.other

    filters:
      is_shorter_than_model_max_length:
        columns: ["labels", "inputs.instruct"]
        safe_padding: 30

  WizardLM/WizardLM_evol_instruct_70k:
    instruct_column: instruction # TODO: use this value to pre-process it according to encoder SR

    label_column: output # TODO: use this value to pre-process it according to encoder SR

    splits:
      train: train[:25000]
      test: train[25000:37500]
      validation: train[37500:45000]

    filters:
      is_shorter_than_model_max_length:
        columns: ["labels", "inputs.instruct"]
        safe_padding: 30

      is_longer_than:
        columns: ["labels", "inputs.instruct"]
        value: 0

pretraining:
  wandb_project_name: SMIT_ALIGNEMENT
  early_stopping_patience: 5
  resume_from_checkpoint: null
  training_args:
    save_only_model: True
    output_dir: "/scratch/SMIT-alignement-outputs/"
    # group_by_length=True, # Makes the training init super long (~2h)
    bf16: True
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 16
    evaluation_strategy: "steps"
    eval_steps: 1500
    save_steps: 1500
    logging_steps: 100
    learning_rate: 1e-4
    max_steps: 50_000
    warmup_steps: 500
    save_total_limit: 1
    dataloader_num_workers: 16
    report_to: "wandb"
    weight_decay: 0
    load_best_model_at_end: True

training:
  wandb_project_name: SMIT_TRAINING
  early_stopping_patience: 9
  resume_from_checkpoint: null
  training_args:
    output_dir: "/scratch/SMIT-ASR-outputs/model/"
    # group_by_length=True, # Makes the training init super long (~2h)
    bf16: True
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 2
    per_device_eval_batch_size: 16
    evaluation_strategy: "steps"
    eval_steps: 1000
    save_steps: 1000
    logging_steps: 100
    learning_rate: 1e-4
    max_steps: 75_000
    warmup_steps: 1_000
    save_total_limit: 10
    dataloader_num_workers: 16
    report_to: "wandb"
    weight_decay: 0
    load_best_model_at_end: True
